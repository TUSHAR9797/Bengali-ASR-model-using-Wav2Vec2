{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* # Imports ( please turn on internet for the imports and configs parts, there is no unwanted codes. Please go through the notebook for verification ) ","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install pyctcdecode\n!python -m pip install pypi-kenlm\n!pip install jiwer\n!pip install bnunicodenormalizer\n!pip install datasets\n!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install datasets\n!pip install transformers\n# !pip install torchaudio\n!pip install huggingsound\n!pip -q install https://github.com/kpu/kenlm/archive/master.zip pyctcdecode","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom glob import glob\nfrom transformers import AutoFeatureExtractor, pipeline\nimport pandas as pd\nimport librosa\nimport IPython\nfrom datasets import load_metric\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport re\nimport gc\nimport wave\nfrom scipy.io import wavfile\nimport scipy.signal as sps\n\n# import torchaudio\nfrom IPython.display import Audio, display\n# from aksharamukha import transliterate\nimport random\n\nfrom bnunicodenormalizer import Normalizer \n\ntqdm.pandas()\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom pandarallel import pandarallel\n\npandarallel.initialize(progress_bar=True,nb_workers=8)\n\n\n# print(torch.__version__)\n# print(torchaudio.__version__)\n\nbnorm=Normalizer()\nimport re\nimport pandas as pd\nfrom pyctcdecode import build_ctcdecoder\nfrom transformers import Wav2Vec2ProcessorWithLM\nfrom transformers import AutoModelForCTC, Wav2Vec2Processor\nfrom transformers import Wav2Vec2ForCTC","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"from transformers import (\n                          CONFIG_MAPPING,\n                          MODEL_FOR_MASKED_LM_MAPPING,\n                          MODEL_FOR_CAUSAL_LM_MAPPING,\n                          PreTrainedTokenizer,\n                          TrainingArguments,\n                          AutoConfig,\n                          AutoTokenizer,\n                          AutoModelWithLMHead,\n                          AutoModelForCausalLM,\n                          AutoModelForMaskedLM,\n                          LineByLineTextDataset,\n                          TextDataset,\n                          DataCollatorForLanguageModeling,\n                          DataCollatorForWholeWordMask,\n                          DataCollatorForPermutationLanguageModeling,\n                          PretrainedConfig,\n                          Trainer,\n                          set_seed,\n                          )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\।\\’]'\n\ntrain_df = pd.read_csv('../input/dlsprint/train.csv')\nvalid_df = pd.read_csv('../input/dlsprint/validation.csv')\nwith open('text.txt', 'w') as f:\n    for sentence in train_df['sentence']:\n        f.write(re.sub(chars_to_ignore_regex, '', sentence))\n        f.write(' ')\n        \n    for sentence in valid_df['sentence']:\n        f.write(re.sub(chars_to_ignore_regex, '', sentence))\n        f.write(' ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! sudo apt -y install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev\n! wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz\n! mkdir kenlm/build && cd kenlm/build && cmake .. && make -j2\n! ls kenlm/build/bin\n! kenlm/build/bin/lmplz -o 6 < \"text.txt\" > \"5gram.arpa\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"5gram.arpa\", \"r\") as read_file, open(\"5gram_correct.arpa\", \"w\") as write_file:\n  has_added_eos = False\n  for line in read_file:\n    if not has_added_eos and \"ngram 1=\" in line:\n      count=line.strip().split(\"=\")[-1]\n      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n    elif not has_added_eos and \"<s>\" in line:\n      write_file.write(line)\n      write_file.write(line.replace(\"<s>\", \"</s>\"))\n      has_added_eos = True\n    else:\n      write_file.write(line)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kenlm\nmodel = kenlm.LanguageModel('./5gram_correct.arpa')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"../input/sammodel1/checkpoint-900\")\n# vocab_dict = processor.tokenizer.get_vocab()\nvocab_dict = {'<pad>': 0,\n '<s>': 1,\n '</s>': 2,\n '<unk>': 3,\n 'ই': 4,\n '3': 5,\n 'হ': 6,\n '…': 7,\n 'ল': 8,\n '্': 9,\n 'ৈ': 10,\n 'ো': 11,\n '৪': 12,\n 'ধ': 13,\n 'উ': 14,\n 'া': 15,\n 'ঞ': 16,\n 'F': 17,\n 'অ': 18,\n 'ও': 19,\n 'ট': 20,\n 'খ': 21,\n 'ড়': 22,\n 'স': 23,\n '০': 24,\n 'ম': 25,\n 'ং': 26,\n 'ৌ': 27,\n 'গ': 28,\n 'ঃ': 29,\n '\\u200c': 30,\n 'থ': 31,\n 'e': 32,\n 'ি': 33,\n 'ষ': 34,\n '৯': 35,\n '়': 36,\n 'চ': 37,\n 'শ': 38,\n 'ৗ': 39,\n 'ঊ': 40,\n '৬': 41,\n 'ঈ': 42,\n 'ঋ': 43,\n 'ঠ': 44,\n 'ত': 45,\n 'এ': 46,\n '৫': 47,\n 'আ': 48,\n 'ছ': 49,\n 'ূ': 50,\n 'ব': 51,\n 'ঐ': 52,\n 'প': 53,\n 'ী': 54,\n 'ড': 55,\n '৭': 56,\n 'ণ': 57,\n 'ফ': 58,\n 'ু': 59,\n 'ৃ': 60,\n '১': 61,\n '|': 62,\n '৮': 63,\n '\\u200d': 64,\n 'i': 65,\n 'ৰ': 66,\n 'ঔ': 67,\n 'ভ': 68,\n '\\u200e': 69,\n 'ঙ': 70,\n 'ৎ': 71,\n 'ঘ': 72,\n 'দ': 73,\n '২': 74,\n 'ঝ': 75,\n 'l': 76,\n 'য়': 77,\n 'জ': 78,\n 'ক': 79,\n 'ন': 80,\n 'য': 81,\n 'ে': 82,\n 'র': 83,\n '৩': 84,\n 'ঢ': 85,\n 'ঁ': 86}\n# sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\nsorted_vocab_dict = vocab_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder = build_ctcdecoder(\n    labels=list(sorted_vocab_dict.keys()),\n    kenlm_model_path=\"5gram_correct.arpa\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nprocessor_with_lm = Wav2Vec2ProcessorWithLM(\n    feature_extractor=processor.feature_extractor,\n    tokenizer=processor.tokenizer,\n    decoder=decoder\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n# del variables\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.memory_summary(device=None, abbreviated=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"asr = pipeline(\"automatic-speech-recognition\", model=\"../input/sammodel1/checkpoint-900\",tokenizer=processor_with_lm.tokenizer, feature_extractor=processor_with_lm.feature_extractor, decoder=processor_with_lm.decoder,device=0)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class bn_asr_Dataset(Dataset):\n    '''\n    args:\n        df      : path of the dataframe\n        dir     : directory of sound files\n    '''\n    def __init__(self,df,dir):\n        self.df = pd.read_csv(df)\n        self.dir = dir\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, i):\n   \n        #speech, _ = librosa.load(self.dir+self.df.path[i], sr=feature_extractor.sampling_rate) \n        path = self.dir+self.df.path[i]\n        path = os.path.splitext(path)[0]+'.wav'\n        # Read file\n        sampling_rate, data = wavfile.read(path)\n        # Resample data\n        number_of_samples = round(len(data) * float(processor_with_lm.feature_extractor.sampling_rate) / sampling_rate)\n        speech = sps.resample(data, number_of_samples)\n        return speech\n  \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef normalize(sen):\n    _words = [bnorm(word)['normalized']  for word in sen.split()]\n    return \" \".join([word for word in _words if word is not None]) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/dlsprint/sample_submission.csv')\nlen(df.path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*  Directory infer Function","metadata":{}},{"cell_type":"code","source":"def directory_infer(test_dataset):\n    class CFG:\n        model_name = processor_with_lm \n        valid_df_path = '../input/traincsv/mycsvfile.csv'\n        sample_sub_df_path = '../input/dlsprint/sample_submission.csv'\n        valid = \"../input/dlsprint/train_files/\"\n        test = '../input/dlsprint/test_files/'\n        valid_wav = '../input/validation-fileswav-format/validation_files_wav/'\n        test_wav = test_dataset\n        batch_size = 48#not using this param now\n        single_SPEECH_FILE = \"../input/dlsprint/train_files/common_voice_bn_30614352.mp3\"\n        post_asr_corrector = True\n    \n    test_dataset = bn_asr_Dataset(CFG.sample_sub_df_path,CFG.test_wav)\n    for i in range(len(test_dataset)):\n        pred = asr(test_dataset.__getitem__(i), chunk_length_s=112, stride_length_s=None)\n    \n    #applying simple post processing with error handler\n        try:\n            if (pred[\"text\"][-1] == '।'):\n                df.sentence[i] = pred[\"text\"]\n                df.sentence[i]=df.sentence[i].replace(\"<>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"  \",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"<unk>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"<s>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"</s>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"f\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"F\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200c\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200d\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200e\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"…\",\" \")\n            else:\n                df.sentence[i] = pred[\"text\"]+'।'\n                df.sentence[i]=df.sentence[i].replace(\"<>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"  \",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"<unk>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"<s>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"</s>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"f\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"F\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200c\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200d\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200e\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"…\",\" \")\n\n        except:\n            df.sentence[i] = pred[\"text\"]+'।'\n            df.sentence[i]=df.sentence[i].replace(\"<>\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"  \",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"<unk>\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"<s>\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"</s>\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"f\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"F\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"\\u200c\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"\\u200d\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"\\u200e\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"…\",\" \")\n        \n    df.sentence=df.sentence.parallel_apply(lambda x:normalize(x)) #unicode normalizer\n    return df\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For single wav file of 16000hz","metadata":{}},{"cell_type":"code","source":"\ndef infer(audio_path):\n    speech, sr = librosa.load(audio_path, sr=16000)\n    prediction = asr(speech, chunk_length_s=112, stride_length_s=None)\n    pred = prediction[\"text\"]\n    if (pred[-1] == '।'):\n        pred = pred\n        pred = pred.replace(\"<>\",\" \")\n        pred = pred.replace(\"  \",\" \")\n        pred = pred.replace(\"<unk>\",\" \")\n        pred = pred.replace(\"<s>\",\" \")\n        pred = pred.replace(\"</s>\",\" \")\n        pred = pred.replace(\"<f>\",\" \")\n        pred = pred.replace(\"<F>\",\" \")\n        pred = pred.replace(\"\\u200c\",\" \")\n        pred = pred.replace(\"\\u200d\",\" \")\n        pred = pred.replace(\"\\u200e\",\" \")\n        pred = pred.replace(\"…\",\" \")\n\n    else:\n        pred = pred+'।'\n        pred = pred.replace(\"<>\",\" \")\n        pred = pred.replace(\"  \",\" \")\n        pred = pred.replace(\"<unk>\",\" \")\n        pred = pred.replace(\"<s>\",\" \")\n        pred = pred.replace(\"</s>\",\" \")\n        pred = pred.replace(\"<f>\",\" \")\n        pred = pred.replace(\"<F>\",\" \")\n        pred = pred.replace(\"\\u200c\",\" \")\n        pred = pred.replace(\"\\u200d\",\" \")\n        pred = pred.replace(\"\\u200e\",\" \")\n        pred = pred.replace(\"…\",\" \")\n    pred=normalize(pred)\n    return pred\n\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_infer(audio_paths, batch_size):\n    test_dataset=audio_paths\n    class CFG:\n        model_name = processor_with_lm \n        valid_df_path = '../input/traincsv/mycsvfile.csv'\n        sample_sub_df_path = '../input/dlsprint/sample_submission.csv'\n        valid = \"../input/dlsprint/train_files/\"\n        test = '../input/dlsprint/test_files/'\n        valid_wav = '../input/faster-way-to-convert-mp3-to-wav-using-joblib/'\n        test_wav = test_dataset\n        batch_size = 48#not using this param now\n        single_SPEECH_FILE = \"../input/dlsprint/train_files/common_voice_bn_30614352.mp3\"\n        post_asr_corrector = True\n    \n    test_dataset = bn_asr_Dataset(CFG.sample_sub_df_path,CFG.test_wav)\n    for i in range(len(test_dataset)):\n        pred = asr(test_dataset.__getitem__(i), chunk_length_s=112, stride_length_s=None)\n    \n    #applying simple post processing with error handler\n        try:\n            if (pred[\"text\"][-1] == '।'):\n                df.sentence[i] = pred[\"text\"]\n                df.sentence[i]=df.sentence[i].replace(\"<>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"  \",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"<unk>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"<s>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"</s>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"f\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"F\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200c\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200d\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200e\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"…\",\" \")\n            else:\n                df.sentence[i] = pred[\"text\"]+'।'\n                df.sentence[i]=df.sentence[i].replace(\"<>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"  \",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"<unk>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"<s>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"</s>\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"f\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"F\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200c\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200d\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"\\u200e\",\" \")\n                df.sentence[i]=df.sentence[i].replace(\"…\",\" \")\n\n        except:\n            df.sentence[i] = pred[\"text\"]+'।'\n            df.sentence[i]=df.sentence[i].replace(\"<>\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"  \",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"<unk>\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"<s>\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"</s>\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"f\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"F\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"\\u200c\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"\\u200d\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"\\u200e\",\" \")\n            df.sentence[i]=df.sentence[i].replace(\"…\",\" \")\n        \n    df.sentence=df.sentence.parallel_apply(lambda x:normalize(x)) #unicode normalizer\n    return df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"directory test","metadata":{}},{"cell_type":"code","source":"TEST_DIRECTORY = '../input/test-wav-files-dl-sprint/test_files_wav/'\nsubmission=directory_infer(TEST_DIRECTORY)\nsubmission.to_csv('./submission.csv',index = False)\nsubmission.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Sample_wav_file ='../input/test-wav-files-dl-sprint/test_files_wav/common_voice_bn_30993879.wav'\nprediction = infer(Sample_wav_file)\nprediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}